# @package _global_
# Benchmarking: Solver Timing Comparison
#
# For individual solver benchmarks, use:
#   uv run python main.py +experiment=benchmarking/timings_fv -m    # Finite Volume
#   uv run python main.py +experiment=benchmarking/timings_sg -m    # Spectral Single Grid
#   uv run python main.py +experiment=benchmarking/timings_fsg -m   # Spectral FSG (n_levels=1,2,3)
#
# This config runs a quick comparison across all three solver types at fixed N:
#   uv run python main.py +experiment=benchmarking/timings -m

defaults:
  - override /hydra/launcher: joblib

# MLflow experiment
experiment_name: LDC-Benchmarking
sweep_name: timing-comparison-Re${Re}

# Default problem parameters
Re: 100
N: 24
tolerance: 1.0e-6
max_iterations: 500000

hydra:
  sweeper:
    params:
      solver: fv,spectral/sg,spectral/fsg
  launcher:
    n_jobs: 3
